---
title: "DATA_621_HW3"
author: "Chi Pong, Euclid Zhang, Jie Zou, Joseph Connolly, LeTicia Cancel"
date: "3/8/2022"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r include=FALSE}
library("corrplot")
library("MASS")
library("ggplot2")
library("patchwork")
library("faraway")
library("car")
library("pROC")
library("caret")
library("dplyr")

```


```{r}
train_df <- read.csv("https://raw.githubusercontent.com/ezaccountz/DATA_621/main/HW3/crime-training-data_modified.csv")
test_df <- read.csv("https://raw.githubusercontent.com/ezaccountz/DATA_621/main/HW3/crime-evaluation-data_modified.csv")
```


# DATA EXPLORATION

```{r}
summary(train_df)
```

```{r fig.height=6, fig.width=10}
par(mfrow=c(3,4))
predictors <- colnames(train_df)
predictors <- predictors[!predictors %in% c("target")]

for(preditor in predictors) {
  boxplot(train_df[,preditor],main=preditor)
}
```


```{r fig.height=10, fig.width=10}
par(mfrow=c(4,3))
predictors <- colnames(train_df)
predictors <- predictors[!predictors %in% c("target")]

for(preditor in predictors) {
  boxplot(train_df[,preditor]~train_df$target,main=preditor, 
          xlab = "target", ylab = preditor)
}
```



```{r fig.height=6, fig.width=10}
par(mfrow=c(3,4))

predictors <- colnames(train_df)
predictors <- predictors[!predictors %in% c("target")]

for(preditor in predictors) {
  plot(density(train_df[,preditor],na.rm=TRUE),main=preditor)
}
```


## * **Correlations**

Now let's look at the correlations between the variables  
```{r fig.height=10, fig.width=10}
corrplot(cor(train_df, use = "na.or.complete"), method = 'number', type = 'lower', diag = FALSE, tl.srt = 0.1)
```




# DATA PREPARATION

From the density plot of **zn**, we know that the variable is zero-inflated. The percentage of 0 values is

```{r}
nrow(train_df[train_df$zn==0,])/nrow(train_df)
```
Let's check the distribution of the **zn** without the 0 values

```{r}
plot(density(train_df[train_df$zn>0,]$zn,na.rm=TRUE), main = "zn > 0")
```
The distribution looks a lot better.  
We will add a new dummy variable zn_y indicating if **zn** is >0. The interaction **zn x zn_y = zn** so we don't need to do anything to it. If **zn_y** is deemed to be insignificant by our models, then we can simply drop it.


```{r}
train_df$zn_y <- 0
train_df$zn_y[train_df$zn>0] <- 1
```








According to the text book *A Modern Approach To Regression With R*, "when the predictor variable X has a Poisson distribution, the log odds are a linear function of x". Let's check if any of the predictors follows a Poisson distribution

```{r}
#Method of possion distribution test is from https://stackoverflow.com/questions/59809960/how-do-i-know-if-my-data-fit-a-poisson-distribution-using-r

#two tail test
p_poisson <- function(x) {
  return (1-2 * abs((1 - pchisq((sum((x - mean(x))^2)/mean(x)), length(x) - 1))-0.5))
}

predictors <- colnames(train_df)
predictors <- predictors[!predictors %in% c("target","chas","zn_y")]

data.frame(mean = round(apply(train_df[,predictors],2,mean),2), 
           variance = round(apply(train_df[,predictors],2,var),2),
           probability_of_poisson = round(apply(train_df[,predictors],2,p_poisson),2))
```
None of the predictors follows a poisson distribution



```{r fig.height=6, fig.width=10}
target_factored <- as.factor(train_df$target)

plot_zn <- ggplot(train_df, aes(x=zn, color=target_factored)) + geom_density()
plot_indus <- ggplot(train_df, aes(x=indus, color=target_factored)) + geom_density()
plot_nox <- ggplot(train_df, aes(x=nox, color=target_factored)) + geom_density()
plot_rm <- ggplot(train_df, aes(x=rm, color=target_factored)) + geom_density()
plot_age <- ggplot(train_df, aes(x=age, color=target_factored)) + geom_density()
plot_dis <- ggplot(train_df, aes(x=dis, color=target_factored)) + geom_density()
plot_rad <- ggplot(train_df, aes(x=rad, color=target_factored)) + geom_density()
plot_tax <- ggplot(train_df, aes(x=tax, color=target_factored)) + geom_density()
plot_prtatio <- ggplot(train_df, aes(x=ptratio, color=target_factored)) + geom_density()
plot_lstat <- ggplot(train_df, aes(x=lstat, color=target_factored)) + geom_density()
plots_medv <- ggplot(train_df, aes(x=medv, color=target_factored)) + geom_density()

plot_zn+plot_indus+plot_nox+plot_rm+plot_age+plot_dis+plot_rad+plot_tax+
  plot_prtatio+plot_lstat+plots_medv+plot_layout(ncol = 4, guides = "collect")
```

The distributions for rm with target = 0 and target = 1 are approximately normal with the same variance. Hence we don't need to transform the variable
The distributions for lstat and medv are skewed, we will add a log-transformed variable for each of them
The distributions for indus, nox, age, dis, tax, ptratio look significantly different for the target values. We will not perform transformation for them unless transformations have to be done to meet the model assumptions


```{r}
train_df$log_lstat <- log(train_df$lstat)
train_df$log_medv <- log(train_df$medv)
```


```{r warning=FALSE}
predictors <- colnames(train_df)
predictors <- predictors[!predictors %in% c("target","chas","zn_y","log_lstat","log_medv")]

interaction_test <- data.frame(matrix(ncol = 3, nrow = 0))
colnames(interaction_test) <- c("Preditor","Interaction","p-value")
class(interaction_test$`p-value`) = "Numeric"

for (predictor in predictors) {
  interaction_test[nrow(interaction_test) + 1,] <- 
    c(predictor, paste0(predictor, ":chas"), 
      round(anova(glm(target ~ train_df[,predictor]*chas,data = train_df, family = "binomial"),test="Chi")[4,5],4))
}
```
```{r}
interaction_test
```
We will add an interaction between **tax** and **chas** and an interaction between **rad** and **chas** to our preditor candidates.
```{r}
train_df$tax_chas <- train_df$tax * train_df$chas
train_df$rad_chas <- train_df$rad * train_df$chas 
```




```{r warning=FALSE}
full_model <- glm(target~.,family = binomial, train_df)
```

```{r}
summary(full_model)
```
```{r warning=FALSE}
model_AIC <- step(full_model, trace=0)
```

```{r}
summary(model_AIC)
```


```{r warning=FALSE}
drop1(glm(target ~ .-rad_chas-zn-log_medv-rm-log_lstat-lstat-indus, family=binomial, train_df), test="Chi")
```
```{r warning=FALSE}
model_p <- glm(target ~ .-rad_chas-zn-log_medv-rm-log_lstat-lstat-indus, family=binomial, train_df)
```

```{r}
summary(model_p)
```
```{r}
model_p <- glm(target ~ .-rad_chas-zn-log_medv-rm-log_lstat-lstat-indus-chas-tax_chas, family=binomial, train_df)
```

```{r}
summary(model_p)
```

```{r warning=FALSE}
model_AIC_2 <- step(glm(target~.-zn_y,family = binomial, train_df), trace=0)
```

```{r}
summary(model_AIC_2)
```

```{r}
model_p_2 <- glm(target ~ zn + chas + nox + age + dis + rad + tax + 
    ptratio + lstat + medv + log_lstat + tax_chas-tax_chas-chas-log_lstat-lstat, family=binomial, train_df)
```

```{r}
summary(model_p_2)
```

```{r}
model_compare <- data.frame(
    model = c("full_model","model_AIC","model_p","model_AIC_2","model_p_2"),
    Deviance = rep(0.0000,5),
    AIC = rep(0.0000,5),
    Accurarcy = rep(0.0000,5),
    Sensitivity = rep(0.0000,5),
    Specificity = rep(0.0000,5),
    Precision = rep(0.0000,5),
    F1 = rep(0.0000,5),
    AUC = rep(0.0000,5),
    Nagelkerke_R_squared = rep(0.0000,5)
)
```
```{r message=FALSE, warning=FALSE}
models <- list(full_model, model_AIC, model_p, model_AIC_2, model_p_2)

for (i in c(1:5)) {
  predicted_class <- ifelse(models[[i]]$fitted.values>0.5,1,0)
  confusion_matrix <- confusionMatrix(as.factor(predicted_class),
                                      as.factor(train_df$target),positive = "1")
  
  model_compare[i,] <- c(model_compare[i,1],round(models[[i]]$deviance,4), models[[i]]$aic, 
                            confusion_matrix$overall[1],
                            confusion_matrix$byClass[1],
                            confusion_matrix$byClass[2],
                            confusion_matrix$byClass[3],
                            2*confusion_matrix$byClass[1]*confusion_matrix$byClass[3]/
                              (confusion_matrix$byClass[1]+confusion_matrix$byClass[3]),
                            auc(roc(train_df$target, models[[i]]$fitted.values)),
                            (1-exp((models[[i]]$dev-models[[i]]$null)/
                                     length(models[[i]]$residuals)))/
                              (1-exp(-models[[i]]$null/length(models[[i]]$residuals)))
                            )
}

```

```{r}
model_compare[,c(2:10)] <- sapply(model_compare[,c(2:10)],as.numeric)
model_compare
```




```{r fig.height=10, fig.width=10, message=FALSE, warning=FALSE}
marginalModelPlots(model_AIC,~nox+age+dis+rad+tax+ptratio+medv,layout =c(3,3))
```


```{r}

residual_df <- mutate(train_df, residuals=residuals(model_AIC,type="deviance"), linpred=predict(model_AIC,type = "link"))

gdf <- group_by(residual_df, cut(linpred, breaks=unique(quantile(linpred,(1:100)/101))))

diagdf <- summarise(gdf, residuals=mean(residuals), linpred=mean(linpred))

plot(residuals ~ linpred, diagdf, xlab="linear predictor",xlim=c(-20,20))
```
```{r}
model_AIC
```



```{r}

predictors <- c("nox","age","dis","rad","tax","ptratio","medv")

residual_df <- mutate(train_df, residuals=residuals(model_AIC,type="deviance"))
gg_plots <- list()

for (i in c(1:length(predictors))) {
    gdf <- group_by(residual_df, .dots = predictors[i])
    diagdf <- summarise(gdf, residuals=mean(residuals))
    print(ggplot(diagdf, aes_string(x=predictors[i],y="residuals")) + geom_point())
}
```



```{r}
halfnorm(hatvalues(model_AIC))
```
```{r}
train_df[c(210,235),]
```






```{r}
test_df$zn_y <- 0
test_df$zn_y[test_df$zn>0] <- 1

test_df$log_lstat <- log(test_df$lstat)
test_df$log_medv <- log(test_df$medv)

test_df$tax_chas <- test_df$tax * test_df$chas
test_df$rad_chas <- test_df$rad * test_df$chas 
```


```{r}
test_df$predicted_class <- ifelse(predict(model_AIC,test_df, type = "response") >0.5,1,0)
```



```{r}
hist(test_df$predicted_class, main = "model_AIC prediction", xlab="predicted value")
```